{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import nltk \n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n36 - Class Label\\n40 - Sentence\\n'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Reading the Dataset (ISEAR Dataset)\n",
    "'''\n",
    "Data = pd.read_csv('my_table.csv',header=None)\n",
    "'''\n",
    "36 - Class Label\n",
    "40 - Sentence\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Emotion Labels\n",
    "'''\n",
    "emotion_labels = ['joy', 'fear', 'anger', 'sadness', 'disgust', 'shame', 'guilt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Negation words\n",
    "'''\n",
    "negation_words = ['not', 'neither', 'nor', 'but', 'however', 'although', 'nonetheless', 'despite', 'except', 'even though', 'yet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Returns a list of all corresponding class labels\n",
    "'''\n",
    "def class_labels(emotions):\n",
    "    labels = []\n",
    "    for e in emotions:\n",
    "        labels.append(e)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Removes unnecessary characters from sentences\n",
    "'''\n",
    "def removal(sentences):\n",
    "    sentence_list = []\n",
    "    count = 0\n",
    "    for sen in sentences:\n",
    "        count += 1\n",
    "#         print count\n",
    "#         print sen\n",
    "#         print type(sen)\n",
    "        s = nltk.word_tokenize(sen)\n",
    "        characters = [\"รก\", \"\\xc3\", \"\\xa1\", \"\\n\", \",\", \".\"]\n",
    "        new = ' '.join([i for i in s if not [e for e in characters if e in i]])\n",
    "        sentence_list.append(new)\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "POS-TAGGER, returns NAVA words\n",
    "'''\n",
    "def pos_tag(sentences):\n",
    "    tags = []\n",
    "    nava_sen = []\n",
    "    for s in sentences:\n",
    "        s_token = nltk.word_tokenize(s)\n",
    "        pt = nltk.pos_tag(s_token)\n",
    "        nava = []\n",
    "        nava_words = []\n",
    "        for t in pt:\n",
    "            if t[1].startswith('NN') or t[1].startswith('JJ') or t[1].startswith('VB') or t[1].startswith('RB'):\n",
    "                nava.append(t)\n",
    "                nava_words.append(t[0])\n",
    "        tags.append(nava)\n",
    "        nava_sen.append(nava_words)\n",
    "    return tags, nava_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Performs stemming\n",
    "'''\n",
    "def stemming(sentences):\n",
    "    sentence_list = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for sen in sentences:\n",
    "        st = \"\"\n",
    "        for word in sen:\n",
    "            word_l = word.lower()\n",
    "            if len(word_l) >= 3:\n",
    "                st += stemmer.stem(word_l) + \" \"\n",
    "        w_set = nltk.word_tokenize(st)\n",
    "        w_text = nltk.Text(w_set)\n",
    "        sentence_list.append(w_text)\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Creating the dataframe\n",
    "'''\n",
    "def create_frame(Data):\n",
    "    emotions = Data[36]\n",
    "    sit = Data[40]\n",
    "    labels = class_labels(emotions[1:50])\n",
    "    sent = removal(sit[1:50])\n",
    "    nava, sent_pt = pos_tag(sent)\n",
    "    sentences = stemming(sent_pt)\n",
    "    frame = pd.DataFrame({0 : labels,\n",
    "                          1 : sentences})\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = create_frame(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Reads the emotion representative words file\n",
    "'''\n",
    "def readfile(filename):\n",
    "    f = open(filename,'r')\n",
    "    representative_words = []\n",
    "    for line in f.readlines():\n",
    "        characters = [\"\\n\", \" \", \"\\r\", \"\\t\"]\n",
    "        new = ''.join([i for i in line if not [e for e in characters if e in i]])\n",
    "        representative_words.append(new)\n",
    "    return representative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Makes a list of all words semantically related to an emotion and Stemming\n",
    "'''\n",
    "def affect_wordlist(words):\n",
    "    affect_words = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for w in words:\n",
    "        w_l = w.lower()\n",
    "        word_stem = stemmer.stem(w_l)\n",
    "        if word_stem not in affect_words:\n",
    "            affect_words.append(word_stem)\n",
    "    return affect_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Creating an emotion wordnet\n",
    "'''\n",
    "def emotion_word_set(emotions):\n",
    "    word_set = {}\n",
    "    for e in emotions:\n",
    "        representative_words = readfile(e)\n",
    "        wordlist = affect_wordlist(representative_words)\n",
    "        word_set[e] = wordlist\n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_textbody(sentences):\n",
    "    for sen in sentences:\n",
    "        print sen\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Lexicon based approach - Check for lexicons\n",
    "'''\n",
    "def lexicon_based(sentences, word_set):\n",
    "    text_vector = []\n",
    "    for sen in sentences:\n",
    "        s_vector = []\n",
    "        for word in sen:\n",
    "            w_vector = {}\n",
    "            for emo in word_set:\n",
    "                if word in word_set[emo]:\n",
    "                    print word\n",
    "                    try:\n",
    "                        if emo not in w_vector[word]:\n",
    "                            w_vector[word].append(emo)\n",
    "                    except KeyError:\n",
    "                        w_vector[word] = [emo]\n",
    "            if w_vector:\n",
    "                s_vector.append(w_vector)\n",
    "        if not s_vector:\n",
    "            text_vector.append(s_vector)\n",
    "        else:\n",
    "            text_vector.append(s_vector)\n",
    "    return text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Lexicon based approach - Classify based on lexicons\n",
    "'''\n",
    "def classify_lexicon(text_vector, labels, emotion_labels):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for j in range(len(text_vector)):\n",
    "        sen = text_vector[j]\n",
    "        sen_emo = np.empty(len(emotion_labels))\n",
    "        sen_emo.fill(0)\n",
    "        if sen:\n",
    "            total += 1\n",
    "            w_emo = []\n",
    "            for word in sen:\n",
    "                emotions =  word.values()[0][0]\n",
    "                print emotions, type(emotions), j\n",
    "                w_emo.append(emotions)\n",
    "                i = emotion_labels.index(emotions)\n",
    "                sen_emo[i] += 1\n",
    "            print sen_emo\n",
    "            winner = np.argwhere(sen_emo == np.amax(sen_emo))\n",
    "            indices = winner.flatten().tolist()\n",
    "            for i in indices:\n",
    "                if emotion_labels[i] == labels[j]:\n",
    "                    count += 1\n",
    "                    break\n",
    "                else:\n",
    "                    print j, text_vector[j]\n",
    "    accuracy = count/len(text_vector)\n",
    "    tot_accuracy = count/total\n",
    "    return accuracy, tot_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "infuri\n",
      "lost\n",
      "nauseou\n",
      "afraid\n",
      "peac\n",
      "love\n",
      "live\n",
      "low\n",
      "low\n",
      "mean\n",
      "humili\n",
      "joy\n",
      "wrong\n",
      "accus\n",
      "stupid\n",
      "good\n",
      "lost\n",
      "lost\n",
      "bad\n",
      "guilt\n",
      "guilt\n",
      "guilt\n",
      "guilt\n",
      "accus\n",
      "fail\n",
      "fail\n",
      "sick\n",
      "joy <type 'str'> 0\n",
      "[ 1.  0.  0.  0.  0.  0.  0.]\n",
      "anger <type 'str'> 2\n",
      "[ 0.  0.  1.  0.  0.  0.  0.]\n",
      "sadness <type 'str'> 3\n",
      "[ 0.  0.  0.  1.  0.  0.  0.]\n",
      "disgust <type 'str'> 4\n",
      "[ 0.  0.  0.  0.  1.  0.  0.]\n",
      "fear <type 'str'> 8\n",
      "[ 0.  1.  0.  0.  0.  0.  0.]\n",
      "joy <type 'str'> 14\n",
      "[ 1.  0.  0.  0.  0.  0.  0.]\n",
      "joy <type 'str'> 15\n",
      "[ 1.  0.  0.  0.  0.  0.  0.]\n",
      "15 [{u'love': ['joy']}]\n",
      "joy <type 'str'> 17\n",
      "[ 1.  0.  0.  0.  0.  0.  0.]\n",
      "17 [{u'live': ['joy']}]\n",
      "shame <type 'str'> 18\n",
      "[ 0.  0.  0.  0.  0.  1.  0.]\n",
      "18 [{u'low': ['shame', 'sadness']}]\n",
      "shame <type 'str'> 22\n",
      "[ 0.  0.  0.  0.  0.  1.  0.]\n",
      "22 [{u'mean': ['shame']}]\n",
      "shame <type 'str'> 26\n",
      "[ 0.  0.  0.  0.  0.  1.  0.]\n",
      "joy <type 'str'> 28\n",
      "[ 1.  0.  0.  0.  0.  0.  0.]\n",
      "guilt <type 'str'> 29\n",
      "[ 0.  0.  0.  0.  0.  0.  1.]\n",
      "29 [{u'wrong': ['guilt']}]\n",
      "guilt <type 'str'> 30\n",
      "[ 0.  0.  0.  0.  0.  0.  1.]\n",
      "30 [{u'accus': ['guilt']}]\n",
      "anger <type 'str'> 33\n",
      "[ 0.  0.  1.  0.  0.  0.  0.]\n",
      "33 [{u'stupid': ['anger']}]\n",
      "joy <type 'str'> 34\n",
      "[ 1.  0.  0.  0.  0.  0.  0.]\n",
      "34 [{u'good': ['joy']}]\n",
      "sadness <type 'str'> 38\n",
      "[ 0.  0.  0.  1.  0.  0.  0.]\n",
      "38 [{u'lost': ['sadness']}]\n",
      "sadness <type 'str'> 39\n",
      "[ 0.  0.  0.  1.  0.  0.  0.]\n",
      "39 [{u'lost': ['sadness']}]\n",
      "sadness <type 'str'> 42\n",
      "shame <type 'str'> 42\n",
      "[ 0.  0.  0.  1.  0.  1.  0.]\n",
      "42 [{u'bad': ['sadness']}, {u'guilt': ['shame', 'guilt']}]\n",
      "shame <type 'str'> 43\n",
      "[ 0.  0.  0.  0.  0.  1.  0.]\n",
      "43 [{u'guilt': ['shame', 'guilt']}]\n",
      "guilt <type 'str'> 46\n",
      "[ 0.  0.  0.  0.  0.  0.  1.]\n",
      "46 [{u'accus': ['guilt']}]\n",
      "sadness <type 'str'> 47\n",
      "[ 0.  0.  0.  1.  0.  0.  0.]\n",
      "disgust <type 'str'> 48\n",
      "[ 0.  0.  0.  0.  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "e = emotion_word_set(emotion_labels)\n",
    "l = lexicon_based(c[1],e) \n",
    "a, b = classify_lexicon(l, c[0], emotion_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Calculate pmi\n",
    "'''\n",
    "def pmi(x, y, sentences):\n",
    "    count_x = 1\n",
    "    count_y = 1\n",
    "    count_xy = 1\n",
    "    for sen in sentences:\n",
    "        if x and y in sentences:\n",
    "            count_xy += 1\n",
    "            count_x += 1\n",
    "            count_y += 1\n",
    "        if x in sentences:\n",
    "            count_x += 1\n",
    "        if y in sentences:\n",
    "            count_y += 1\n",
    "        result = count_xy/(count_x * count_y)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.224489795918\n",
      "0.478260869565\n"
     ]
    }
   ],
   "source": [
    "print a\n",
    "print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'joy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-b7fca851cf09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m '''\n\u001b[0;32m      4\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0memo_word_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memotion_word_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memotion_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-39-f3e22985b7bd>\u001b[0m in \u001b[0;36memotion_word_set\u001b[1;34m(emotions)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mword_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0memotions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mrepresentative_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreadfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mwordlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maffect_wordlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepresentative_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mword_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordlist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-fd8daf1d7eee>\u001b[0m in \u001b[0;36mreadfile\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      3\u001b[0m '''\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreadfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mrepresentative_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'joy'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Emotion Detector\n",
    "'''\n",
    "c = create_frame(Data)\n",
    "emo_word_net = emotion_word_set(emotion_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'shame']\n",
      "[u'shame', u'disgrace', u'ignominy']\n",
      "[u'pity', u'shame']\n",
      "[u'dishonor', u'disgrace', u'dishonour', u'attaint', u'shame']\n",
      "[u'shame']\n",
      "[u'shame']\n",
      "[u'shame']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Getting synonyms from wordnet synsets\n",
    "'''\n",
    "from nltk.corpus import wordnet as wn\n",
    "jw = wn.synsets('shame')\n",
    "for s in jw:\n",
    "    v = s.name()\n",
    "    print wn.synset(v).lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
