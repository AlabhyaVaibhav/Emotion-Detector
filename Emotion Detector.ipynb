{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import nltk \n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n36 - Class Label\\n40 - Sentence\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Reading the Dataset (ISEAR Dataset)\n",
    "'''\n",
    "Data = pd.read_csv('my_table.csv',header=None)\n",
    "'''\n",
    "36 - Class Label\n",
    "40 - Sentence\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Emotion Labels\n",
    "'''\n",
    "emotion_labels = ['joy', 'fear', 'anger', 'sadness', 'disgust', 'shame', 'guilt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Negation words\n",
    "'''\n",
    "negation_words = ['not', 'neither', 'nor', 'but', 'however', 'although', 'nonetheless', 'despite', 'except', 'even though', 'yet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Returns a list of all corresponding class labels\n",
    "'''\n",
    "def class_labels(emotions):\n",
    "    labels = []\n",
    "    for e in emotions:\n",
    "        labels.append(e)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Removes unnecessary characters from sentences\n",
    "'''\n",
    "def removal(sentences):\n",
    "    sentence_list = []\n",
    "    count = 0\n",
    "    for sen in sentences:\n",
    "        count += 1\n",
    "#         print count\n",
    "#         print sen\n",
    "#         print type(sen)\n",
    "        s = nltk.word_tokenize(sen)\n",
    "        characters = [\"รก\", \"\\xc3\", \"\\xa1\", \"\\n\", \",\", \".\"]\n",
    "        new = ' '.join([i for i in s if not [e for e in characters if e in i]])\n",
    "        sentence_list.append(new)\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "POS-TAGGER, returns NAVA words\n",
    "'''\n",
    "def pos_tag(sentences):\n",
    "    tags = []\n",
    "    nava_sen = []\n",
    "    for s in sentences:\n",
    "        s_token = nltk.word_tokenize(s)\n",
    "        pt = nltk.pos_tag(s_token)\n",
    "        nava = []\n",
    "        nava_words = []\n",
    "        for t in pt:\n",
    "            if t[1].startswith('NN') or t[1].startswith('JJ') or t[1].startswith('VB') or t[1].startswith('RB'):\n",
    "                nava.append(t)\n",
    "                nava_words.append(t[0])\n",
    "        tags.append(nava)\n",
    "        nava_sen.append(nava_words)\n",
    "    return tags, nava_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Performs stemming\n",
    "'''\n",
    "def stemming(sentences):\n",
    "    sentence_list = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for sen in sentences:\n",
    "        st = \"\"\n",
    "        for word in sen:\n",
    "            word_l = word.lower()\n",
    "            if len(word_l) >= 3:\n",
    "                st += stemmer.stem(word_l) + \" \"\n",
    "        w_set = nltk.word_tokenize(st)\n",
    "        w_text = nltk.Text(w_set)\n",
    "        sentence_list.append(w_text)\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Creating the dataframe\n",
    "'''\n",
    "def create_frame(Data):\n",
    "    emotions = Data[36]\n",
    "    sit = Data[40]\n",
    "    labels = class_labels(emotions[1:50])\n",
    "    sent = removal(sit[1:50])\n",
    "    nava, sent_pt = pos_tag(sent)\n",
    "    sentences = stemming(sent_pt)\n",
    "    frame = pd.DataFrame({0 : labels,\n",
    "                          1 : sentences})\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = create_frame(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Reads the emotion representative words file\n",
    "'''\n",
    "def readfile(filename):\n",
    "    f = open(filename,'r')\n",
    "    representative_words = []\n",
    "    for line in f.readlines():\n",
    "        characters = [\"\\n\", \" \", \"\\r\", \"\\t\"]\n",
    "        new = ''.join([i for i in line if not [e for e in characters if e in i]])\n",
    "        representative_words.append(new)\n",
    "    return representative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Makes a list of all words semantically related to an emotion and Stemming\n",
    "'''\n",
    "def affect_wordlist(words):\n",
    "    affect_words = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for w in words:\n",
    "        w_l = w.lower()\n",
    "        word_stem = stemmer.stem(w_l)\n",
    "        if word_stem not in affect_words:\n",
    "            affect_words.append(word_stem)\n",
    "    return affect_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Creating an emotion wordnet\n",
    "'''\n",
    "def emotion_word_set(emotions):\n",
    "    word_set = {}\n",
    "    for e in emotions:\n",
    "        representative_words = readfile(e)\n",
    "        wordlist = affect_wordlist(representative_words)\n",
    "        word_set[e] = wordlist\n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Lexicon based approach - Check for lexicons\n",
    "'''\n",
    "def lexicon_based(sentences, word_set):\n",
    "    text_vector = []\n",
    "    for sen in sentences:\n",
    "        s_vector = []\n",
    "        for word in sen:\n",
    "            w_vector = {}\n",
    "            for emo in word_set:\n",
    "                if word in word_set[emo]:\n",
    "#                     print word\n",
    "                    try:\n",
    "                        if emo not in w_vector[word]:\n",
    "                            w_vector[word].append(emo)\n",
    "                    except KeyError:\n",
    "                        w_vector[word] = [emo]\n",
    "            if w_vector:\n",
    "                s_vector.append(w_vector)\n",
    "        if not s_vector:\n",
    "            text_vector.append(s_vector)\n",
    "        else:\n",
    "            text_vector.append(s_vector)\n",
    "    return text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Lexicon based approach - Classify based on lexicons\n",
    "'''\n",
    "def classify_lexicon(text_vector, labels, emotion_labels):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for j in range(len(text_vector)):\n",
    "        sen = text_vector[j]\n",
    "        sen_emo = np.empty(len(emotion_labels))\n",
    "        sen_emo.fill(0)\n",
    "        if sen:\n",
    "            total += 1\n",
    "            w_emo = []\n",
    "            for word in sen:\n",
    "                emotions =  word.values()[0][0]\n",
    "#                 print emotions, type(emotions), j\n",
    "                w_emo.append(emotions)\n",
    "                i = emotion_labels.index(emotions)\n",
    "                sen_emo[i] += 1\n",
    "#             print sen_emo\n",
    "            winner = np.argwhere(sen_emo == np.amax(sen_emo))\n",
    "            indices = winner.flatten().tolist()\n",
    "            for i in indices:\n",
    "                if emotion_labels[i] == labels[j]:\n",
    "                    count += 1\n",
    "                    break\n",
    "#                 else:\n",
    "#                     print j, text_vector[j]\n",
    "    accuracy = count/len(text_vector)\n",
    "    tot_accuracy = count/total\n",
    "    return accuracy, tot_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e = emotion_word_set(emotion_labels)\n",
    "l = lexicon_based(c[1],e) \n",
    "a, b = classify_lexicon(l, c[0], emotion_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Calculate pmi\n",
    "'''\n",
    "def pmi(x, y, sentences):\n",
    "    count_x = 1\n",
    "    count_y = 1\n",
    "    count_xy = 1\n",
    "    for sen in sentences:\n",
    "        if x and y in sentences:\n",
    "            count_xy += 1\n",
    "            count_x += 1\n",
    "            count_y += 1\n",
    "        if x in sentences:\n",
    "            count_x += 1\n",
    "        if y in sentences:\n",
    "            count_y += 1\n",
    "        result = count_xy/(count_x * count_y)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.4489795918 %\n",
      "47.8260869565 %\n"
     ]
    }
   ],
   "source": [
    "print a*100, '%'\n",
    "print b*100, \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Emotion Detector\n",
    "'''\n",
    "c = create_frame(Data)\n",
    "emo_word_net = emotion_word_set(emotion_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'shame']\n",
      "[u'shame', u'disgrace', u'ignominy']\n",
      "[u'pity', u'shame']\n",
      "[u'dishonor', u'disgrace', u'dishonour', u'attaint', u'shame']\n",
      "[u'shame']\n",
      "[u'shame']\n",
      "[u'shame']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Getting synonyms from wordnet synsets\n",
    "'''\n",
    "from nltk.corpus import wordnet as wn\n",
    "jw = wn.synsets('shame')\n",
    "for s in jw:\n",
    "    v = s.name()\n",
    "    print wn.synset(v).lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
